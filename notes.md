## 2017-06

## Layer normalization
- batch normalization adapted to RNN

## Attention Is All You Need [???]
keywords: transformer, self attention, multi-head attention, encoder-decoder
- eschews RNN and CNN and uses positional embedding to provide positional information
- composes feed forward NN exclusively with attention to learn representation between input and output
- transformer model architecture [picture]
- multi head attention
? good for seq2seq tasks

## Label Smoothing
- a form of regularization that penalizes low entropy predictions


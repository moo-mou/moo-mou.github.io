<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>ML paper notes - moomou</title><link rel="icon" type="image/png" href=favicon.ico /><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="ML paper notes" />
<meta property="og:description" content="2017-09 LEARNING FINE-GRAINED IMAGE SIMILARITY WITH DEEP RANKING describes efficient sampling technique based on reservoir sampling for building triplets; requires an relevance function multi scale CNN DEEP METRIC LEARNING USING TRIPLET NETWORK learns a semantic embedding; results show better discrimination vs siamese network (contrastive loss function) MSE softmax shows improved performance rather than simple binary softmax (see paper for def) feed a triplet of x, x1, x2 where x1 is same class as x and x2 is different DISTILLING THE KNOWLEDGE IN A NEURAL NETWORK explores compression technique of ensemble model into a single model Distillation softmax qi = exp(zi/T)/Sigma(j)(exp(zj/T) where z are logits and T is temperature T is usually 1 increasing T creates softer probability distribution knowledge is tranferred via training smaller/compressed model by targeting over softer target (ie temperature T &gt; 1) from more cumbersome model small model trained with higher T as well but in prediction mode uses T = 1 tranfer training can be improved by using datasets with true label demonstrate distillation with minist dataset - tranfer works well even when smaller model trained by omitting certain numbers discusses using soft distribution target technique for training specialists on very large datasets Google internal JFT data of 100M images Questions teacher - student model, relation to curriculum learning?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://paul.mou.dev/notes/ml_notes/" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2017-09-01T23:01:25-07:00" />
<meta property="article:modified_time" content="2017-09-01T23:01:25-07:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ML paper notes"/>
<meta name="twitter:description" content="2017-09 LEARNING FINE-GRAINED IMAGE SIMILARITY WITH DEEP RANKING describes efficient sampling technique based on reservoir sampling for building triplets; requires an relevance function multi scale CNN DEEP METRIC LEARNING USING TRIPLET NETWORK learns a semantic embedding; results show better discrimination vs siamese network (contrastive loss function) MSE softmax shows improved performance rather than simple binary softmax (see paper for def) feed a triplet of x, x1, x2 where x1 is same class as x and x2 is different DISTILLING THE KNOWLEDGE IN A NEURAL NETWORK explores compression technique of ensemble model into a single model Distillation softmax qi = exp(zi/T)/Sigma(j)(exp(zj/T) where z are logits and T is temperature T is usually 1 increasing T creates softer probability distribution knowledge is tranferred via training smaller/compressed model by targeting over softer target (ie temperature T &gt; 1) from more cumbersome model small model trained with higher T as well but in prediction mode uses T = 1 tranfer training can be improved by using datasets with true label demonstrate distillation with minist dataset - tranfer works well even when smaller model trained by omitting certain numbers discusses using soft distribution target technique for training specialists on very large datasets Google internal JFT data of 100M images Questions teacher - student model, relation to curriculum learning?"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://paul.mou.dev/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://paul.mou.dev/css/main.css" /><link rel="stylesheet" type="text/css" href="https://paul.mou.dev/css/dark.css"  />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="https://paul.mou.dev/js/main.js"></script>
	<script src="https://paul.mou.dev/js/abc.js"></script>
	<script src="https://paul.mou.dev/js/xyz.js"></script>
	<script src="https://code.jquery.com/jquery-3.4.1.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title"><a href="https://paul.mou.dev/">moomou</a></h1>
	<div class="site-description"><h2>(ﾉ≧∇≦)ﾉ ﾐ ┸┸</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/moomou" title="Github"><i data-feather="github"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/posts">All posts</a>
			</li>
			
			<li>
				<a href="https://go.mou.dev/resume">Resume</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">ML paper notes</h1>
      
      <div class="meta">Posted at &mdash; Sep 1, 2017</div>
      
      
		</div>

		<div class="markdown">
			<h3 id="2017-09">2017-09</h3>
<h2 id="learning-fine-grained-image-similarity-with-deep-ranking">LEARNING FINE-GRAINED IMAGE SIMILARITY WITH DEEP RANKING</h2>
<ul>
<li>describes efficient sampling technique based on reservoir sampling for building triplets; requires an relevance function</li>
<li>multi scale CNN</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="deep-metric-learning-using-triplet-network">DEEP METRIC LEARNING USING TRIPLET NETWORK</h2>
<ul>
<li>learns a semantic embedding; results show better discrimination vs siamese network (contrastive loss function)</li>
<li>MSE softmax shows improved performance rather than simple binary softmax (see paper for def)</li>
<li>feed a triplet of x, x1, x2 where x1 is same class as x and x2 is different</li>
</ul>
<h2 id="distilling-the-knowledge-in-a-neural-network">DISTILLING THE KNOWLEDGE IN A NEURAL NETWORK</h2>
<ul>
<li>explores compression technique of ensemble model into a single model</li>
<li>Distillation
<ul>
<li>softmax qi = exp(zi/T)/Sigma(j)(exp(zj/T) where z are logits and T is temperature</li>
</ul>
<ul>
<li>T is usually 1</li>
<li>increasing T creates softer probability distribution</li>
</ul>
<ul>
<li>knowledge is tranferred via training smaller/compressed model by targeting over softer target (ie temperature T &gt; 1) from more cumbersome model</li>
<li>small model trained with higher T as well but in prediction mode uses T = 1</li>
<li>tranfer training can be improved by using datasets with true label</li>
</ul>
</li>
<li>demonstrate distillation with minist dataset - tranfer works well even when smaller model trained by omitting certain numbers</li>
<li>discusses using soft distribution target technique for training specialists on very large datasets</li>
<li>Google internal JFT data of 100M images</li>
</ul>
<h4 id="questions">Questions</h4>
<ul>
<li>
<p>teacher - student model, relation to curriculum learning?</p>
</li>
<li>
<p>Feels like specialist training is a separate topic from distillation</p>
</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="regularizing-neural-networks-by-penalizing-confident-output-distributions">REGULARIZING NEURAL NETWORKS BY PENALIZING CONFIDENT OUTPUT DISTRIBUTIONS</h2>
<ul>
<li>using entropy as an extra regularizer term to improve model generalizability</li>
<li>shows improved performance across wide variety of classification tasks</li>
</ul>
<h4 id="questions-1">Questions</h4>
<ul>
<li>label smoothing regularization</li>
</ul>
<!-- raw HTML omitted -->
<h3 id="2017-08">2017-08</h3>
<h2 id="voco-text-based-insertion-and-replacement-in-audio-narration">VOCO: TEXT-BASED INSERTION AND REPLACEMENT IN AUDIO NARRATION</h2>
<p>keywords: voice conversion, t2s</p>
<ul>
<li>uses short, non annotated corpus of speech data to synthesize t2s</li>
<li>builds on CUTE and requires small corpus</li>
</ul>
<h3 id="question">Question</h3>
<ul>
<li>CUTE: <a href="http://gfx.cs.princeton.edu/pubs/Jin_2016_CAC/CUTE-icassp_2016.pdf">http://gfx.cs.princeton.edu/pubs/Jin_2016_CAC/CUTE-icassp_2016.pdf</a></li>
<li>HelpingHand: <a href="http://gfx.cs.princeton.edu/pubs/Lu_2012_HES/Lu_2012_HES.pdf">http://gfx.cs.princeton.edu/pubs/Lu_2012_HES/Lu_2012_HES.pdf</a></li>
<li>forced alignment: <a href="http://www.speech.kth.se/prod/publications/files/908.pdf">http://www.speech.kth.se/prod/publications/files/908.pdf</a></li>
<li>Mel-Cepstral Distortion (MCD):</li>
<li>Dynamic Time Warping (DTW)</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="an-overlap-add-technique-based-on-waveform-similarity-wsola-for-high-quality-time-scale-modification-of-speech">AN OVERLAP-ADD TECHNIQUE BASED ON WAVEFORM SIMILARITY (WSOLA) FOR HIGH QUALITY TIME-SCALE MODIFICATION OF SPEECH</h2>
<p>keywords: voice synthesis, text 2 speech</p>
<h4 id="questions-2">Questions</h4>
<ul>
<li>WSOLA</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="learning-a-predictable-and-generative-vector-representation-for-objects">LEARNING A PREDICTABLE AND GENERATIVE VECTOR REPRESENTATION FOR OBJECTS</h2>
<ul>
<li>a generative embedding for 3D object that also works for 2D</li>
<li>autoencoder for generative and cnn for predictability</li>
<li>joint loss from both input - image + voxel</li>
<li>training performed in 3 stages, 1) autoencoder only, 2) cnn with autoencoder embedding and 3) jointly optimizes with scaled loss function</li>
<li>experimentally verified semantic meaning of the 64D vector by observing effects when changing value in only 1D; compared against PCA to justify nonlinear rep</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="learning-to-compare-image-patches-via-convolutional-neural-networks">LEARNING TO COMPARE IMAGE PATCHES VIA CONVOLUTIONAL NEURAL NETWORKS</h2>
<p>keyword: feature extraction</p>
<ul>
<li>learn a similarity function entirely from data</li>
<li>tested 2 channel, siamese, pseudo-siamese architecture</li>
<li>applied technique from [VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION] to break up large CNN into multi layered CNN with ReLU in between</li>
<li>obtained best result with 2 channel</li>
</ul>
<h4 id="questions-3">Questions</h4>
<ul>
<li>SIFT</li>
<li>SPP</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="data-driven-synthesis-of-smoke-flows-with-cnn-based-feature-descriptors">DATA-DRIVEN SYNTHESIS OF SMOKE FLOWS WITH CNN-BASED FEATURE DESCRIPTORS</h2>
<p>keyword: low dimension feature descriptor</p>
<ul>
<li>???</li>
</ul>
<h3 id="questions-4">Questions</h3>
<ul>
<li>advection step</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="deep-unfolding-model-based-inspiration-of-novel-deep-architectures">DEEP UNFOLDING: MODEL-BASED INSPIRATION OF NOVEL DEEP ARCHITECTURES</h2>
<ul>
<li>deep unfolding - start with a model based approach and then convert iterative step into layers in network; untie the parameter across layers to obtain a trainable model across layers</li>
</ul>
<h3 id="questions-5">Questions</h3>
<ul>
<li>markov random fields vs CRF</li>
<li>belief propagation</li>
<li>variational approximations</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="audio-driven-facial-animation-by-joint-end-to-end-learning-of-pose-and-emotion">AUDIO-DRIVEN FACIAL ANIMATION BY JOINT END-TO-END LEARNING OF POSE AND EMOTION</h2>
<p>keywords: e2e, audio, emotion, lip vertex position</p>
<ul>
<li>uses a feature vector from a fully connected layer as emotional representation;</li>
<li>emotion database is then later assigned a semantic meaning via playback on target mesh and manually assigning semantic label</li>
<li>had to apply specific smoothing and ensemble (same input, averaged result across 2 runs) to obtain temporal stability</li>
<li>uses PCA to pre-initialize the &ldquo;bottleneck&rdquo; layer</li>
<li>defined a loss function of vertex position, vertex motion, and regularization to encourage long term change from emotion and shortterm change from audio</li>
</ul>
<h3 id="questions-6">Questions</h3>
<ul>
<li>Source filter model?</li>
<li>autocorrelation</li>
<li>deformation transfer</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="architectures-for-deep-neural-network-based-acoustic-models-defined-over-windowed-speech-waveforms">ARCHITECTURES FOR DEEP NEURAL NETWORK BASED ACOUSTIC MODELS DEFINED OVER WINDOWED SPEECH WAVEFORMS</h2>
<p>keywords: raw input</p>
<ul>
<li>investigated whether windowed speech wavform (WSW) DNN can be on par with MFCC/MFSC feature based DNN</li>
<li>stacked bottleneck layer showed markable improvemnent for WSW and less for MFCC/MFSC - resulting in similar WER</li>
<li>investigated using trained weight matrix structure as initialization vs random initialization</li>
</ul>
<h3 id="questions-7">Questions</h3>
<ul>
<li>MFSC vs MFCC??</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="spectral-subband-centroid-features-for-speech-recognition">SPECTRAL SUBBAND CENTROID FEATURES FOR SPEECH RECOGNITION</h2>
<ul>
<li>SSC improves baseline recognition performance as supplemnt features for cepstral coefficients</li>
<li>spectral centroid, SSC, etc. are good features to try in additional to MFCC</li>
</ul>
<!-- raw HTML omitted -->
<h3 id="lcn-cnn-dnn-for-text-dependent-sv">LCN, CNN, DNN FOR TEXT DEPENDENT SV</h3>
<ul>
<li>experiment with LCN and CNN as first layer for a DNN text dependent SV system</li>
<li>shows that computation and parameters can be vastly reduced with CNN/LCN without losing too much performance</li>
<li>try simpler layers with fewer parameters first</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="resnet">Resnet</h2>
<p>keywords: skip layer,</p>
<ul>
<li>bottleneck design with skip connection</li>
<li>propagating &ldquo;raw&rdquo; information across layer helps with deeper architecture</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="network-in-a-network">NETWORK IN A NETWORK</h2>
<p>keywords: NIN</p>
<ul>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/5n01i4/d_network_in_network_nin_is_it_still_useful/dc7qfd1/">https://www.reddit.com/r/MachineLearning/comments/5n01i4/d_network_in_network_nin_is_it_still_useful/dc7qfd1/</a></li>
</ul>
<!-- raw HTML omitted -->
<h2 id="automatic-gain-control-and-multi-style-training-for-robust-small-footprint-keyword-spotting-with-deep-neural-networks">AUTOMATIC GAIN CONTROL AND MULTI-STYLE TRAINING FOR ROBUST SMALL-FOOTPRINT KEYWORD SPOTTING WITH DEEP NEURAL NETWORKS</h2>
<p>keywords:  multi-style training, small-footprint models</p>
<ul>
<li>trains a frontend system with EM algorithm on peak power by assuming signals from mixed Gaussian signals</li>
<li>then applies automatic gain control (AGC) to speech segment</li>
<li>builds on the DNN keyword system from &ldquo;Small-footprint key- word spotting using deep neural networks&rdquo;</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="acoustic-modelling-from-the-signal-domain-using-cnns">ACOUSTIC MODELLING FROM THE SIGNAL DOMAIN USING CNNS</h2>
<p>keywords: CNN, raw waveform, statistic extraction layer, Network In Network nonlinearity</p>
<ul>
<li>LVCSR</li>
<li>directly learn the &ldquo;features&rdquo; from raw signals</li>
<li>uses the proposed NIN block to replace RELU and maxpooling</li>
<li>propsed DNN used in combination with traditional iVector approach</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="end-to-end-text-dependent-speaker-verification">END-TO-END TEXT-DEPENDENT SPEAKER VERIFICATION</h2>
<p>keywords: speaker verification, end-to-end training</p>
<ul>
<li>training e2e by using accept or reject as the loss function</li>
<li>LSTM outperforms small footprint DNN but better DNN can improve base performance</li>
<li>utterance vs frame level modeling - utterance level works better</li>
<li>uses locally connected layers</li>
</ul>
<h4 id="questions-8">Questions</h4>
<ul>
<li>Is it important for your system to use frame level modeling? Utterance should work?</li>
<li>What are the locally connected layer advantages?</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="deep-neural-network-based-speaker-embeddings-for-end-to-end-speaker-verification">DEEP NEURAL NETWORK-BASED SPEAKER EMBEDDINGS FOR END-TO-END SPEAKER VERIFICATION</h2>
<p>keywords: speaker vr, text-indepdendent</p>
<ul>
<li>uses network in a network (NIN) architecture to project an input vector (di) into output vector (do)</li>
<li>trains directly on distance metric to create neural network that creates more characteristic embedding</li>
<li>need a large dataset before DNN approach exceeded the ivector baseline</li>
<li>uses US phone data with 102K speaker data</li>
<li>2 network - one for generating speaker embedding and another for maximizing log prob of same speaker pair</li>
<li>&ldquo;curriculum&rdquo; learning of training on long durations then short durations works better</li>
<li>Custom objective function with PLDA inspired distance metrics</li>
</ul>
<h4 id="questions-9">Questions</h4>
<ul>
<li>How well do these results generalize across same speaker but different languages?</li>
<li>DNN + PLDA seems to work better everytime?</li>
</ul>
<!-- raw HTML omitted -->
<h3 id="2017-07">2017-07</h3>
<h2 id="boosted-treeshttpxgboostreadthedocsioenlatestmodelhtml"><a href="http://xgboost.readthedocs.io/en/latest/model.html">BOOSTED TREES</a></h2>
<p>keywords: boosted trees, random trees</p>
<ul>
<li>random forest and tree ensemble are the same except in how they are trained</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="deep-learning-for-hate-speech-detection-in-tweets">DEEP LEARNING FOR HATE SPEECH DETECTION IN TWEETS</h2>
<p>keywords: glove, fasttext, hate speech, GBDT (gradient boosted decision trees)</p>
<ul>
<li>network architecture experiment with various embedding</li>
<li>random embedding</li>
<li>task specific embedding/feature vector useful as input to downstream system</li>
<li>(LSTM + Random Embedding + GBDT) is much better than (CNN + Random Embedding + GBDT)</li>
</ul>
<h4 id="questions-10">Questions</h4>
<ul>
<li>what does random embedding mean exactly? Why does it work better?</li>
<li>How popular DNN + GBDT architecture?</li>
</ul>
<h2 id="a-time-delay-neural-network-architecture-for-efficient-modeling-of-long-temporal-contexts">A TIME DELAY NEURAL NETWORK ARCHITECTURE FOR EFFICIENT MODELING OF LONG TEMPORAL CONTEXTS</h2>
<p>keywords: subsampling, dnn,</p>
<ul>
<li>basically just dilated cnn without the convolution - ie fully connected</li>
<li>[-2,2] means {-2, -1, 0, 1, 2}</li>
<li>asymmetric input splicing - more frames from the past (up to 16 frames) and future (up to 9 frames); more frames prove to be detrimental to word recognition accuracies
Q:
<ul>
<li>Is TDNN similar to dilation CNN?</li>
<li>What is the sampling rate</li>
</ul>
</li>
</ul>
<h2 id="maxout-network-summaryhttpwwwsimon-hohbergde20150719maxouthtml"><a href="http://www.simon-hohberg.de/2015/07/19/maxout.html">MAXOUT NETWORK SUMMARY</a></h2>
<ul>
<li>a form activiation designed for dropout</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="deep-neural-networks-for-small-footprint-text-dependent-speaker-verification">DEEP NEURAL NETWORKS FOR SMALL FOOTPRINT TEXT-DEPENDENT SPEAKER VERIFICATION</h2>
<ul>
<li>???</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="deep-speaker-feature-learning-for-text-independent-speaker-verification">DEEP SPEAKER FEATURE LEARNING FOR TEXT-INDEPENDENT SPEAKER VERIFICATION</h2>
<p>keywords: SR, speaker feature vector</p>
<ul>
<li>experiments suggests speaker feature is a short time phenomenon; achieves ERR of 7.68% from 300ms of frame</li>
<li>uses 2 layer CNN with maxpooling plus TDNN with p norm activation function</li>
<li>trained with 5000 speakers</li>
<li>uses tsne to verify speaker cluster well</li>
</ul>
<h3 id="questions-11">Questions</h3>
<ul>
<li>Is TDNN similar to dilation CNN?</li>
<li>How will the network perform with ReLU instead?</li>
<li>Why did keras remove maxout dense layer?</li>
<li>Is maxout dense basically maxpooling?</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="a-simple-way-to-initialize-recurrent-networks-of-rectified-linear-units">A SIMPLE WAY TO INITIALIZE RECURRENT NETWORKS OF RECTIFIED LINEAR UNITS</h2>
<p>keywords: identity matrix, rnn, lstm</p>
<ul>
<li>using identity matrix initialization for RELU RNN works well with little tuning</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="multiscale-context-aggregation-by-dilated-convolution">MULTISCALE CONTEXT AGGREGATION BY DILATED CONVOLUTION</h2>
<p>keywords: dilated, convolution, segmentation, dense prediction</p>
<ul>
<li>aggregates multiscale information without pooling or sampling</li>
<li>exponetial increase of receptive field with increasing dilation</li>
<li>F_i_+_1 = F_i * 2^i * ki</li>
<li>standard intiailzation didn&rsquo;t show improvement; uses a form of identity intialization</li>
<li>applied context module to a frontend of adapated VGG16 without last pooling and striding(?) layers</li>
<li>context module improved semantic segmentation task</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="denoising-with-wavent">DENOISING WITH WAVENT</h2>
<p>keywords: noncausal dilated convolution, raw signal,</p>
<ul>
<li>energy preserving loss based on receptive field rather than a single sample</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="squeeze-net">SQUEEZE NET</h2>
<p>keywords: deep compression, small model</p>
<ul>
<li>efficient model microarchitecture for building small and competitive image recognition models</li>
<li>less 5MB,</li>
<li>uses Fire module defined by s11, e11, e33</li>
<li>expander and squeeze sub components</li>
<li>competitive and 50x less parameters than alexnets</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="multi-scale-context-aggregation-by-dilated-convolutions">MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS</h2>
<p>keywords: dilated convolution, segmentation, dense prediction</p>
<ul>
<li>using dilated convolution to segment images</li>
<li>identity reinitialization needed to improve accuracy</li>
<li>combination with CRF-RNN improves accuracy further</li>
<li>experiments suggest no good reason to downsample and then upsample - adds complexity and hurts accuracy
to explore: structured prediction, CRF-RNN</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="2017-06">2017-06</h2>
<h2 id="layer-normalization">LAYER NORMALIZATION</h2>
<ul>
<li>batch normalization adapted to RNN</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="attention-is-all-you-need">ATTENTION IS ALL YOU NEED</h2>
<p>keywords: transformer, self attention, multi-head attention, encoder-decoder</p>
<ul>
<li>eschews RNN and CNN and uses positional embedding to provide positional information</li>
<li>composes feed forward NN exclusively with attention to learn representation between input and output</li>
<li>transformer model architecture - see paper for picture</li>
<li>multi head attention</li>
</ul>
<h3 id="questions-12">Questions</h3>
<ul>
<li>Is it good for seq2seq tasks</li>
</ul>
<h2 id="label-smoothing">LABEL SMOOTHING</h2>
<ul>
<li>a form of regularization that penalizes low entropy predictions</li>
</ul>
<!-- raw HTML omitted -->

		</div>

		<div class="post-tags">
			
				
			
		</div>
		</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> since 2017 </div>
	</nav>
</div>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'trackingcode', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script>feather.replace()</script>
</body>
</html>

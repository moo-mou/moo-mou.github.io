<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>moomou</title>
    <link>https://paul.mou.dev/</link>
    <description>Recent content on moomou</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>since 2017</copyright>
    <lastBuildDate>Sun, 31 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://paul.mou.dev/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Listening with LLM</title>
      <link>https://paul.mou.dev/posts/2023-12-31-listening-with-llm/</link>
      <pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://paul.mou.dev/posts/2023-12-31-listening-with-llm/</guid>
      <description>Overview This is the first part of many posts I am writing to consolidate learnings on how to finetune Large Language Models (LLMs) to process audio, with the eventual goal of being able to build and host a LLM able to describe human voices.</description>
    </item>
    
    <item>
      <title>Learning to Debug Gibberish</title>
      <link>https://paul.mou.dev/posts/2023-12-27-ohmy/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://paul.mou.dev/posts/2023-12-27-ohmy/</guid>
      <description>Overview Recently, I went down a rabbit hole of debugging opensource LLM spewing gibberish on my PC. The investigation was the one of the most difficult (but interesting) debugging experience I have encountered so far.</description>
    </item>
    
    <item>
      <title>CORS with Cookie</title>
      <link>https://paul.mou.dev/posts/2022-08-13-cors-with-cookie/</link>
      <pubDate>Sat, 13 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://paul.mou.dev/posts/2022-08-13-cors-with-cookie/</guid>
      <description>Introduction CORS stands for Cross-Origin-Resource-Sharing and is the HTTP mechanism to allow servers to accept requests from other host locations other than its own.</description>
    </item>
    
    <item>
      <title>Messing with Nvidia GPU on Headless Linux</title>
      <link>https://paul.mou.dev/posts/2021-12-29-nvidia-gpus/</link>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://paul.mou.dev/posts/2021-12-29-nvidia-gpus/</guid>
      <description>I have a PC with Ubuntu server installed and Nvidia GPUs attached. I have Googled on and off for a while trying to learn how to overclock the GPUs without success.</description>
    </item>
    
    <item>
      <title>Improvements to &#34;m&#34;, a personal command line tool</title>
      <link>https://paul.mou.dev/posts/dev-ux2/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://paul.mou.dev/posts/dev-ux2/</guid>
      <description>From m to m2 Having used m for many years now, I encountered two primary problems.
The first is startup performance.</description>
    </item>
    
    <item>
      <title>Training a Speaker Embedding from Scratch with Triplet Learning</title>
      <link>https://paul.mou.dev/posts/speaker-embedding/</link>
      <pubDate>Sat, 05 May 2018 23:01:25 -0700</pubDate>
      
      <guid>https://paul.mou.dev/posts/speaker-embedding/</guid>
      <description>Posted on go.mou.dev/triplet-embedding-learning</description>
    </item>
    
    <item>
      <title>ML paper notes</title>
      <link>https://paul.mou.dev/notes/ml_notes/</link>
      <pubDate>Fri, 01 Sep 2017 23:01:25 -0700</pubDate>
      
      <guid>https://paul.mou.dev/notes/ml_notes/</guid>
      <description>2017-09 LEARNING FINE-GRAINED IMAGE SIMILARITY WITH DEEP RANKING describes efficient sampling technique based on reservoir sampling for building triplets; requires an relevance function multi scale CNN DEEP METRIC LEARNING USING TRIPLET NETWORK learns a semantic embedding; results show better discrimination vs siamese network (contrastive loss function) MSE softmax shows improved performance rather than simple binary softmax (see paper for def) feed a triplet of x, x1, x2 where x1 is same class as x and x2 is different DISTILLING THE KNOWLEDGE IN A NEURAL NETWORK explores compression technique of ensemble model into a single model Distillation softmax qi = exp(zi/T)/Sigma(j)(exp(zj/T) where z are logits and T is temperature T is usually 1 increasing T creates softer probability distribution knowledge is tranferred via training smaller/compressed model by targeting over softer target (ie temperature T &amp;gt; 1) from more cumbersome model small model trained with higher T as well but in prediction mode uses T = 1 tranfer training can be improved by using datasets with true label demonstrate distillation with minist dataset - tranfer works well even when smaller model trained by omitting certain numbers discusses using soft distribution target technique for training specialists on very large datasets Google internal JFT data of 100M images Questions teacher - student model, relation to curriculum learning?</description>
    </item>
    
    <item>
      <title>Tools</title>
      <link>https://paul.mou.dev/notes/tools/</link>
      <pubDate>Fri, 01 Sep 2017 23:01:25 -0700</pubDate>
      
      <guid>https://paul.mou.dev/notes/tools/</guid>
      <description>Fzf Amazing command line tool to fuzzy search for files. Cannot live without this. Also integrates with vim.
https://github.com/junegunn/fzf
Jq Swiss army knife for working with JSON on the command line</description>
    </item>
    
    <item>
      <title>Developer Experience</title>
      <link>https://paul.mou.dev/posts/dev-ux/</link>
      <pubDate>Tue, 22 Aug 2017 23:01:25 -0700</pubDate>
      
      <guid>https://paul.mou.dev/posts/dev-ux/</guid>
      <description>Commandline Productivity and Automation (aka make it easy to repeat) Developers tend to repeat themselves. A lot. This can be as innocuous as running a test manually after you update a test file or as insidious as deploying a newly built binary into production manually.</description>
    </item>
    
  </channel>
</rss>
